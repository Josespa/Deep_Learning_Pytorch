{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T00:04:01.907729Z",
     "start_time": "2025-01-30T00:04:01.886290Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T00:04:05.804271Z",
     "start_time": "2025-01-30T00:04:03.600714Z"
    }
   },
   "source": [
    "mnist_train = pd.read_csv('../datasets/mnist-in-csv/mnist_train.csv')\n",
    "mnist_test = pd.read_csv('../datasets/mnist-in-csv/mnist_test.csv')\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# %cd /content/drive/My Drive/Colab Notebooks/\n",
    "# mnist_train = pd.read_csv('datasets/mnist-in-csv/mnist_train.csv')\n",
    "# mnist_test = pd.read_csv('datasets/mnist-in-csv/mnist_test.csv')"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T00:04:06.112339Z",
     "start_time": "2025-01-30T00:04:06.083079Z"
    }
   },
   "source": [
    "mnist_train.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T00:04:06.679366Z",
     "start_time": "2025-01-30T00:04:06.519469Z"
    }
   },
   "source": [
    "mnist_train = mnist_train.dropna()\n",
    "mnist_test = mnist_test.dropna()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Sample\n",
    "* We will use transpose to change the shape of image tensor<br>\n",
    "<b>.imshow()</b> needs a 2D array, or a 3D array with the third dimension being of size 3 or 4 only (For RGB or RGBA), so we will shift first axis to last<br>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T00:04:09.948086Z",
     "start_time": "2025-01-30T00:04:09.937626Z"
    }
   },
   "source": [
    "random_sel = mnist_train.sample(8) # select randomly 8 images\n",
    "\n",
    "random_sel.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 785)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T00:04:13.777592Z",
     "start_time": "2025-01-30T00:04:13.743962Z"
    }
   },
   "source": [
    "image_features = random_sel.drop('label', axis = 1)\n",
    "\n",
    "image_batch = (torch.Tensor(image_features.values / 255.)).reshape((-1, 28, 28)) # normalization\n",
    "\n",
    "image_batch.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T00:04:14.282632Z",
     "start_time": "2025-01-30T00:04:14.263065Z"
    }
   },
   "source": [
    "grid = torchvision.utils.make_grid(image_batch.unsqueeze(1), nrow=8)\n",
    "\n",
    "grid.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 242])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T00:04:14.607511Z",
     "start_time": "2025-01-30T00:04:14.478205Z"
    }
   },
   "source": [
    "plt.figure (figsize = (12, 12))\n",
    "\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "plt.axis('off')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 241.5, 31.5, -0.5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying features and labels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T00:04:47.072272Z",
     "start_time": "2025-01-30T00:04:46.960617Z"
    }
   },
   "source": [
    "mnist_train_features = mnist_train.drop('label', axis =1)\n",
    "mnist_train_target = mnist_train['label']\n",
    "\n",
    "mnist_test_features = mnist_test.drop('label', axis =1)\n",
    "mnist_test_target = mnist_test['label']"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T00:05:02.578423Z",
     "start_time": "2025-01-30T00:05:02.573719Z"
    }
   },
   "cell_type": "code",
   "source": "type(mnist_train_features.values)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### converting to tensors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.715743Z",
     "start_time": "2025-01-29T19:41:49.652272Z"
    }
   },
   "source": [
    "X_train_tensor = torch.tensor(mnist_train_features.values, dtype=torch.float)\n",
    "x_test_tensor  = torch.tensor(mnist_test_features.values, dtype=torch.float) \n",
    "\n",
    "Y_train_tensor = torch.tensor(mnist_train_target.values, dtype=torch.long)\n",
    "y_test_tensor  = torch.tensor(mnist_test_target.values, dtype=torch.long)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.722254Z",
     "start_time": "2025-01-29T19:41:49.716751Z"
    }
   },
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(Y_train_tensor.shape)\n",
    "print(x_test_tensor.shape)\n",
    "print(y_test_tensor.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 784])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping the tensors according to what the CNN needs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.729290Z",
     "start_time": "2025-01-29T19:41:49.723338Z"
    }
   },
   "source": [
    "# first dimension is the batch,-1 means that it depends on whatever values we set\n",
    "# second dimension is channel (grayscale)\n",
    "# third and fourth dimensions are height and width\n",
    "\n",
    "X_train_tensor = X_train_tensor.reshape(-1, 1, 28, 28) \n",
    "\n",
    "x_test_tensor = x_test_tensor.reshape(-1, 1, 28, 28)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.738530Z",
     "start_time": "2025-01-29T19:41:49.730294Z"
    }
   },
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(Y_train_tensor.shape)\n",
    "print(x_test_tensor.shape)\n",
    "print(y_test_tensor.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining  CNN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.747295Z",
     "start_time": "2025-01-29T19:41:49.739918Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring the neural network\n",
    "* The input size will be the channels of the images (in_size)\n",
    "* The final output will have a size equal to the number of classes for the prediction\n",
    "* The convolving kernel will have a size of k_conv_size"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.755307Z",
     "start_time": "2025-01-29T19:41:49.748302Z"
    }
   },
   "source": [
    "# Input size is the number of channels in the input image, for grayscale images this will be 1\n",
    "in_size = 1\n",
    "\n",
    "# first convolutional layer has the size of 16, and so on\n",
    "hid1_size = 16 #Re-run for 32\n",
    "hid2_size = 32 #Re-run for 64\n",
    "\n",
    "# final layer, 10 is the total classes\n",
    "out_size = 10\n",
    "\n",
    "k_conv_size = 5 #re-run for 3"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Convolutional Neural Network\n",
    "\n",
    "<b>Conv2d: </b>Applies a 2D convolution over an input signal composed of several input planes.<br>\n",
    "Parameters<br>\n",
    "in_channels (int) â€“ Number of channels in the input image<br>\n",
    "out_channels (int) â€“ Number of channels produced by the convolution<br>\n",
    "kernel_size (int or tuple) â€“ Size of the convolving kernel<br>\n",
    "\n",
    "<b>BatchNorm2d: </b>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\n",
    "Parameters<br>\n",
    "num_features â€“ C from an expected input of size (N,C,H,W)\n",
    "\n",
    "<b>ReLU: </b>Activation function\n",
    "\n",
    "<b>Maxpool2d: </b>\n",
    "Parameters:<br>\n",
    "kernel_size â€“ the size of the window to take a max over\n",
    "\n",
    "<b>Linear: </b>\n",
    "Parameter:<br>\n",
    "\n",
    "in_features: \n",
    "All the operations above used 4D Tensors of shape \n",
    "\n",
    "Now for fully connected layers(linear layers) we need to transform them in 1D Tensors<br>\n",
    "So to the in_features of fully connected layer we will give size\n",
    "out_features:<br>\n",
    "num_classes = number of output labels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.764419Z",
     "start_time": "2025-01-29T19:41:49.756310Z"
    }
   },
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_size, hid1_size, k_conv_size),\n",
    "            nn.BatchNorm2d(hid1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(hid1_size, hid2_size, k_conv_size),\n",
    "            nn.BatchNorm2d(hid2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        self.fc = nn.Linear(512, out_size)\n",
    "        \n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        print(out.shape)\n",
    "        \n",
    "        out = self.layer2(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        print(out.shape)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        ## F.log_softmax(out, dim=-1)\n",
    "        \n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:49.790819Z",
     "start_time": "2025-01-29T19:41:49.765927Z"
    }
   },
   "source": [
    "model = ConvNet()"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:50.873078Z",
     "start_time": "2025-01-29T19:41:49.794328Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:51.019306Z",
     "start_time": "2025-01-29T19:41:50.873600Z"
    }
   },
   "source": [
    "model.to(device)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:51.066353Z",
     "start_time": "2025-01-29T19:41:51.020314Z"
    }
   },
   "source": [
    "X_train_tensor = X_train_tensor.to(device)\n",
    "x_test_tensor  = x_test_tensor.to(device) \n",
    "\n",
    "Y_train_tensor = Y_train_tensor.to(device)\n",
    "y_test_tensor  = y_test_tensor.to(device)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:41:51.071482Z",
     "start_time": "2025-01-29T19:41:51.067379Z"
    }
   },
   "source": [
    "#Re-run for each different value\n",
    "\n",
    "learning_rate = 0.001 \n",
    "#0.01 \n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "#nn.NLLLoss() \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "#optimizer =torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) \n",
    "                    "
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T19:42:33.659035Z",
     "start_time": "2025-01-29T19:41:51.072005Z"
    }
   },
   "source": [
    "num_epochs = 10\n",
    "loss_values = list()\n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "        \n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs,Y_train_tensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    \n",
    "        print('Epoch - %d, loss - %0.5f '%(epoch, loss.item()))\n",
    "        loss_values.append(loss.item())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 16, 12, 12])\n",
      "torch.Size([60000, 32, 4, 4])\n",
      "torch.Size([60000, 512])\n",
      "torch.Size([60000, 10])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.06 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.40 GiB is allocated by PyTorch, and 4.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs,Y_train_tensor)\n\u001B[0;32m      9\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 10\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep() \n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch - \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m, loss - \u001B[39m\u001B[38;5;132;01m%0.5f\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m%\u001B[39m(epoch, loss\u001B[38;5;241m.\u001B[39mitem()))\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    522\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    523\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m _engine_run_backward(\n\u001B[0;32m    290\u001B[0m     tensors,\n\u001B[0;32m    291\u001B[0m     grad_tensors_,\n\u001B[0;32m    292\u001B[0m     retain_graph,\n\u001B[0;32m    293\u001B[0m     create_graph,\n\u001B[0;32m    294\u001B[0m     inputs,\n\u001B[0;32m    295\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    296\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    297\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    766\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    767\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 768\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    769\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    770\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    771\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    772\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 2.06 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.40 GiB is allocated by PyTorch, and 4.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x = (range(0, 9))\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.plot(x, loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    outputs = model(x_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    y_test = y_test_tensor.cpu().numpy()\n",
    "    predicted = predicted.cpu()\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_score(predicted, y_test))\n",
    "    print(\"Precision: \", precision_score(predicted, y_test, average='weighted'))\n",
    "    print(\"Recall: \", recall_score(predicted, y_test, average='weighted'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using model for predictions "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"sample target data = \", mnist_test_target.values[1005])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample_img = mnist_test_features.values[1005]\n",
    "sample_img = sample_img.reshape(1, 28, 28)\n",
    "\n",
    "sample_img = sample_img[0, :, :]\n",
    "\n",
    "plt.figure(figsize =(6, 6))\n",
    "plt.imshow(sample_img)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample = np.array(mnist_test_features.values[1005]) \n",
    "\n",
    "sample_tensor = torch.from_numpy(sample).float()\n",
    "sample_tensor = sample_tensor.reshape(-1, 1, 28, 28)\n",
    "sample_tensor = sample_tensor.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "y_pred = model(sample_tensor)\n",
    "y_pred"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, predicted = torch.max(y_pred.data, -1)\n",
    "\n",
    "print (\" The predicted label is : \", predicted.item())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.cuda.device_count()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.cuda.current_device()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.cuda.device(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.cuda.get_device_name(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T23:39:38.382282Z",
     "start_time": "2025-01-29T23:39:37.212749Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 1650 with Max-Q Design\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
